{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456e38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "import numpy as np\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27dc9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize components\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    ")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea19d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize Chroma client using the new API\n",
    "client = chromadb.PersistentClient(path=\"./vector_store\")  \n",
    "\n",
    "# Create or get a collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"complaint_embeddings\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# Optional: text splitter and embedding setup\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    ")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a75b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data in chunks\n",
    "def load_data_chunk(file_path, chunk_size=10000):\n",
    "    \"\"\"Load data in chunks to avoid memory issues\"\"\"\n",
    "    return pd.read_csv(file_path, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f401c58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Chunk 1 processed successfully\n",
      "Processing chunk 2\n",
      "Chunk 2 processed successfully\n",
      "Processing chunk 3\n",
      "Chunk 3 processed successfully\n",
      "Processing chunk 4\n",
      "Chunk 4 processed successfully\n",
      "Processing chunk 5\n",
      "Chunk 5 processed successfully\n",
      "Processing chunk 6\n",
      "Chunk 6 processed successfully\n",
      "Processing chunk 7\n",
      "Chunk 7 processed successfully\n",
      "Processing chunk 8\n",
      "Chunk 8 processed successfully\n",
      "Processing chunk 9\n",
      "Chunk 9 processed successfully\n",
      "Processing chunk 10\n",
      "Chunk 10 processed successfully\n",
      "Processing chunk 11\n",
      "Chunk 11 processed successfully\n",
      "Processing chunk 12\n",
      "Chunk 12 processed successfully\n",
      "Processing chunk 13\n",
      "Chunk 13 processed successfully\n",
      "Processing chunk 14\n",
      "Chunk 14 processed successfully\n",
      "Processing chunk 15\n",
      "Chunk 15 processed successfully\n",
      "Processing chunk 16\n",
      "Chunk 16 processed successfully\n",
      "Processing chunk 17\n",
      "Chunk 17 processed successfully\n",
      "Processing chunk 18\n",
      "Chunk 18 processed successfully\n",
      "Processing chunk 19\n",
      "Chunk 19 processed successfully\n",
      "Processing chunk 20\n",
      "Chunk 20 processed successfully\n",
      "Processing chunk 21\n",
      "Chunk 21 processed successfully\n",
      "Processing chunk 22\n",
      "Chunk 22 processed successfully\n",
      "Processing chunk 23\n",
      "Chunk 23 processed successfully\n",
      "Processing chunk 24\n",
      "Chunk 24 processed successfully\n",
      "Processing chunk 25\n",
      "Chunk 25 processed successfully\n",
      "Processing chunk 26\n",
      "Chunk 26 processed successfully\n",
      "Processing chunk 27\n",
      "Chunk 27 processed successfully\n",
      "Processing chunk 28\n",
      "Chunk 28 processed successfully\n",
      "Processing chunk 29\n",
      "Chunk 29 processed successfully\n",
      "Processing chunk 30\n",
      "Chunk 30 processed successfully\n",
      "Processing chunk 31\n",
      "Chunk 31 processed successfully\n",
      "Processing chunk 32\n",
      "Chunk 32 processed successfully\n",
      "Processing chunk 33\n",
      "Chunk 33 processed successfully\n",
      "Processing chunk 34\n",
      "Chunk 34 processed successfully\n",
      "Processing chunk 35\n",
      "Chunk 35 processed successfully\n",
      "Processing chunk 36\n",
      "Chunk 36 processed successfully\n",
      "Processing chunk 37\n",
      "Chunk 37 processed successfully\n",
      "Processing chunk 38\n",
      "Chunk 38 processed successfully\n",
      "Processing chunk 39\n",
      "Chunk 39 processed successfully\n",
      "Processing chunk 40\n",
      "Chunk 40 processed successfully\n",
      "Processing chunk 41\n",
      "Chunk 41 processed successfully\n",
      "Processing chunk 42\n",
      "Chunk 42 processed successfully\n",
      "Processing chunk 43\n",
      "Chunk 43 processed successfully\n",
      "Processing chunk 44\n",
      "Chunk 44 processed successfully\n",
      "Processing chunk 45\n",
      "Chunk 45 processed successfully\n",
      "Processing chunk 46\n",
      "Chunk 46 processed successfully\n",
      "Processing chunk 47\n",
      "Chunk 47 processed successfully\n",
      "Processing chunk 48\n",
      "Chunk 48 processed successfully\n",
      "Processing chunk 49\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "ValueError: Batch size of 6531 is greater than max batch size of 5461",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Store in vector database\u001b[39;00m\n\u001b[0;32m     38\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(embeddings))]\n\u001b[1;32m---> 39\u001b[0m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:89\u001b[0m, in \u001b[0;36mCollection.add\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m add_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_prepare_add_request(\n\u001b[0;32m     81\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m     82\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m     uris\u001b[38;5;241m=\u001b[39muris,\n\u001b[0;32m     87\u001b[0m )\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadatas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muris\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\senta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\chromadb\\api\\rust.py:407\u001b[0m, in \u001b[0;36mRustBindingsAPI._add\u001b[1;34m(self, ids, collection_id, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_add\u001b[39m(\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    395\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[0;32m    396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproduct_telemetry_client\u001b[38;5;241m.\u001b[39mcapture(\n\u001b[0;32m    398\u001b[0m         CollectionAddEvent(\n\u001b[0;32m    399\u001b[0m             collection_uuid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m         )\n\u001b[0;32m    405\u001b[0m     )\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: ValueError: Batch size of 6531 is greater than max batch size of 5461"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "data_path = os.path.join(os.path.dirname(os.getcwd()), 'data', 'complaints.csv')\n",
    "chunks = load_data_chunk(data_path)\n",
    "\n",
    "for i, chunk_df in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}\")\n",
    "    \n",
    "    # Filter to keep only necessary columns\n",
    "    chunk_df = chunk_df[['Complaint ID', 'Product', 'Consumer complaint narrative']]\n",
    "    \n",
    "    # Skip rows without narrative\n",
    "    chunk_df = chunk_df[chunk_df['Consumer complaint narrative'].notna()]\n",
    "    \n",
    "    # Split text\n",
    "    all_chunks = []\n",
    "    for _, row in chunk_df.iterrows():\n",
    "        metadata = row.to_dict()\n",
    "        text = row['Consumer complaint narrative']\n",
    "        chunks = splitter.split_text(text)\n",
    "        all_chunks.extend([{\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": metadata\n",
    "        } for chunk in chunks])\n",
    "    \n",
    "    # Extract texts and metadata\n",
    "    texts = [chunk['text'] for chunk in all_chunks]\n",
    "    metadatas = [chunk['metadata'] for chunk in all_chunks]\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    batch_size = 32\n",
    "    embeddings = []\n",
    "    for j in range(0, len(texts), batch_size):\n",
    "        batch = texts[j:j + batch_size]\n",
    "        batch_embeddings = embedder.encode(batch, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    # Store in vector database\n",
    "    ids = [str(i) for i in range(len(embeddings))]\n",
    "    collection.add(\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    \n",
    "    print(f\"Chunk {i + 1} processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Split text\n",
    " \n",
    "for _, row in chunk_df.iterrows():\n",
    "        metadata = row.to_dict()\n",
    "        text = row['consumer_complaint_narrative']\n",
    "        chunks = splitter.split_text(text)\n",
    "        all_chunks.extend([{\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": metadata\n",
    "        } for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97860596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts and metadata\n",
    "texts = [chunk['text'] for chunk in all_chunks]\n",
    "metadatas = [chunk['metadata'] for chunk in all_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6711b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Generate embeddings in batches\n",
    "batch_size = 32\n",
    "embeddings = []\n",
    "for j in range(0, len(texts), batch_size):\n",
    "        batch = texts[j:j + batch_size]\n",
    "        batch_embeddings = embedder.encode(batch, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0429b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in vector database\n",
    "ids = [str(i) for i in range(len(embeddings))]\n",
    "collection.add(\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "print(f\"Chunk {i + 1} processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar complaints\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123006ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i + 1}:\")\n",
    "    print(f\"Similarity Score: {result['scores'][0][i]:.4f}\")\n",
    "    print(f\"Product: {result['metadatas'][0][i]['product']}\")\n",
    "    print(f\"Complaint: {result['documents'][0][i]}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
